{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19694,"status":"ok","timestamp":1715035410420,"user":{"displayName":"Aiden Miner","userId":"12927180881346383813"},"user_tz":360},"id":"H0f6TA26nsZa","outputId":"fe7ef4ab-fd86-4597-b4d2-0679eb956ea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount drive to retrieve dataset\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQBs16Zb1ZOL","executionInfo":{"status":"ok","timestamp":1715046059388,"user_tz":360,"elapsed":34833,"user":{"displayName":"Aiden Miner","userId":"12927180881346383813"}},"outputId":"21f215f8-d747-45e0-f450-c752820b34b0"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Found 5250 images belonging to 2 classes.\n","Found 1750 images belonging to 2 classes.\n","Found 3000 images belonging to 2 classes.\n","Epoch 1/10\n","54/54 [==============================] - 1141s 20s/step - loss: 1.8725 - accuracy: 0.5055 - val_loss: 0.6932 - val_accuracy: 0.5013\n","Epoch 2/10\n","54/54 [==============================] - 933s 17s/step - loss: 0.6933 - accuracy: 0.4933 - val_loss: 0.6929 - val_accuracy: 0.5592\n","Epoch 3/10\n","54/54 [==============================] - 889s 16s/step - loss: 0.6927 - accuracy: 0.5281 - val_loss: 0.6911 - val_accuracy: 0.5247\n","Epoch 4/10\n","54/54 [==============================] - 928s 17s/step - loss: 0.6838 - accuracy: 0.5665 - val_loss: 0.6735 - val_accuracy: 0.5807\n","Epoch 5/10\n","54/54 [==============================] - 938s 17s/step - loss: 0.6436 - accuracy: 0.6455 - val_loss: 0.9120 - val_accuracy: 0.5195\n","Epoch 6/10\n","54/54 [==============================] - 884s 16s/step - loss: 0.6212 - accuracy: 0.6689 - val_loss: 0.6697 - val_accuracy: 0.6283\n","Epoch 7/10\n","54/54 [==============================] - 930s 17s/step - loss: 0.5061 - accuracy: 0.7597 - val_loss: 0.6466 - val_accuracy: 0.6335\n","Epoch 8/10\n","54/54 [==============================] - 944s 17s/step - loss: 0.4030 - accuracy: 0.8139 - val_loss: 0.6922 - val_accuracy: 0.6960\n","Epoch 9/10\n","54/54 [==============================] - 929s 17s/step - loss: 0.3342 - accuracy: 0.8505 - val_loss: 0.6718 - val_accuracy: 0.6999\n","Epoch 10/10\n","54/54 [==============================] - 941s 17s/step - loss: 0.2169 - accuracy: 0.9153 - val_loss: 0.7554 - val_accuracy: 0.7077\n"," 28/300 [=>............................] - ETA: 17:17 - loss: 0.7516 - accuracy: 0.7051"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 300 batches). You may need to use the repeat() function when building your dataset.\n"]},{"output_type":"stream","name":"stdout","text":["300/300 [==============================] - 107s 345ms/step - loss: 0.7516 - accuracy: 0.7051\n","Validation Accuracy: 0.7051428556442261\n","47/47 [==============================] - 902s 20s/step\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.54      0.52      1500\n","           1       0.50      0.46      0.48      1500\n","\n","    accuracy                           0.50      3000\n","   macro avg       0.50      0.50      0.50      3000\n","weighted avg       0.50      0.50      0.50      3000\n","\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 256, 256, 32)      2432      \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 128, 128, 32)      0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 128, 128, 64)      51264     \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 64, 64, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," flatten (Flatten)           (None, 262144)            0         \n","                                                                 \n"," dense (Dense)               (None, 512)               134218240 \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 32)                16416     \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 66        \n","                                                                 \n","=================================================================\n","Total params: 134288418 (512.27 MB)\n","Trainable params: 134288418 (512.27 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["import pandas as pd\n","from keras.optimizers import SGD\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import classification_report\n","\n","# Define the directories\n","train_dir = '/content/drive/MyDrive/unzippedData/rvf10k/train'\n","valid_dir = '/content/drive/MyDrive/unzippedData/rvf10k/valid'\n","\n","# Image data generator\n","datagen = ImageDataGenerator(rescale=1./255, validation_split=0.25)\n","\n","# Flow from directory for training and validation data\n","train_generator = datagen.flow_from_directory(train_dir,\n","                                              target_size=(256, 256),\n","                                              batch_size=64,\n","                                              class_mode='categorical',\n","                                              subset='training')\n","\n","valid_generator = datagen.flow_from_directory(train_dir,\n","                                              target_size=(256, 256),\n","                                              batch_size=64,\n","                                              class_mode='categorical',\n","                                              subset='validation')\n","\n","test_generator = datagen.flow_from_directory(valid_dir,\n","                                             target_size=(256, 256),\n","                                             batch_size=64,\n","                                             class_mode='categorical'\n","                                            )\n","\n","\n","\n","# Define the model\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(256,256,3)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(64, kernel_size=(5,5), padding='same', activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile model\n","sgd = SGD(learning_rate=0.01, weight_decay=0.00001, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Fit model\n","model_fit = model.fit(train_generator,\n","                    steps_per_epoch=7000/128,\n","                    epochs=10,\n","                    validation_data=valid_generator,\n","                    validation_steps=3000/128)\n","\n","# Evaluate model\n","loss, accuracy = model.evaluate(valid_generator, steps=300)\n","print(\"Validation Accuracy:\", accuracy)\n","\n","predictions = model.predict(test_generator)\n","\n","#Prediction of test (valid folder) with classification_report, got from chatGPT\n","predicted_classes = predictions.argmax(axis=1)\n","true_labels = test_generator.classes\n","report = classification_report(true_labels, predicted_classes)\n","print(\"Classification Report:\")\n","print(report)\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"nbcaWOtytsO_"},"source":["1. My project is about detecting images of ai generated faces and photos of real humans; the kaggle data set was made to help address the problems of deepfakes online.. https://www.kaggle.com/datasets/sachchitkunichetty/rvf10k\n","2. All of the project is my own work including design of the model: batch_size, convolutional layers, max pooling layers, dropout, optimizers, training split; except for the prediction of the test data was chatGPT(3.5).\n","3. pandas, keras: .models, .layers, .preprocessing.image, sklearn: .metrics\n","4. I used the CNN model, as I have used it succesfully before, and it is the most popular NN for image classification\n","5. Batch Size crashed at 128 so I wanted to lower it. Different optimizers changed the accuracy:\n","SGD accuracy = 60, adam = 71. I originally did 5 epochs and then saw that the accuracy was moving up steadily, and so was the val_accuracy. I changed it to 10 to allow it to continue to improve and it did up to about epoch 8. After 8 epcohs the val_accuracy wasn't going up enough to warrant more epochs.\n","6. I wanted to use ImageDataGenerator which was very helpful for inputting the images from my drive into the CNN. I found examples in the slides 16_DataAugment_TransferLearning.pdf"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQ+0NAow/YVsGe4qxJLYom"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}